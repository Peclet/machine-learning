{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e5cab749cc865756e1de0545b1eb7a5e71f452f"
   },
   "source": [
    "A simple solution  to https://www.kaggle.com/c/UKYCS460g2018sinkhole.\n",
    "\n",
    "If you would like to make a submission from this notebook, you can follow the instructions here https://www.kaggle.com/dansbecker/submitting-from-a-kernel.  Alternatively, you might want to use https://github.com/Kaggle/kaggle-api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892408519934462"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "train = pd.read_csv('C:/Users/doad224/Downloads/sinkhole-or-not-2019/train.csv')\n",
    "test = pd.read_csv('C:/Users/doad224/Downloads/sinkhole-or-not-2019/test.csv')\n",
    "X=train.drop(columns=['ID','IsSinkhole'])\n",
    "y=train.IsSinkhole\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "#clf = tree.DecisionTreeClassifier(max_depth=8)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "#clf =SVC(kernel='linear')\n",
    "#clf = KNeighborsClassifier(n_jobs=1)\n",
    "#clf = AdaBoostClassifier(n_estimators=100, random_state= 123)\n",
    "#clf1 = DecisionTreeClassifier(max_features= 'auto', min_samples_leaf= 2,max_depth=90, min_samples_split= 10, random_state= 123)\n",
    "#clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.000001,solver='adam', verbose=10,  random_state=21,tol=0.000000001)\n",
    "clf = RandomForestClassifier(min_samples_leaf=0.00000000001,n_jobs=-1,max_features='auto',min_samples_split=4, n_estimators=100, random_state= 123, max_depth=50, bootstrap=False, criterion='entropy')\n",
    "#clf=RandomForestClassifier(min_samples_leaf=0.00000000001,max_features='auto',min_samples_split=4, n_estimators=100, random_state= 123, max_depth=50, bootstrap=False, criterion='entropy')\n",
    "#clf = DecisionTreeClassifier(max_features= 'auto', min_samples_leaf= 2,max_depth=90, min_samples_split= 10, random_state= 123)\n",
    "#clf=LogisticRegression()\n",
    "#clf1.fit(xTrain, yTrain)\n",
    "#clf2.fit(xTrain, yTrain)\n",
    "\n",
    "#eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],voting='soft')\n",
    "clf.fit(xTrain, yTrain)\n",
    "#clf.score(xTrain, yTrain)\n",
    "clf.score(xTest,yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doad2\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters: {'criterion': 'gini', 'max_depth': 9, 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 600, 'n_jobs': -1, 'random_state': 123}\n",
      "Accuracy: 0.8825778263244128\n",
      "Confusion Metrix:\n",
      " [[1301  132]\n",
      " [  83  315]]\n"
     ]
    }
   ],
   "source": [
    "#With Hyper Parameters Tuning\n",
    "#2-2,Randomforest\n",
    "#importing modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#making the instance\n",
    "model=RandomForestClassifier()\n",
    "#hyper parameters set\n",
    "params = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':[100,200,300,600],\n",
    "          'max_depth':[1,2,8,9],\n",
    "          'min_samples_leaf':[1,2,3],\n",
    "          'min_samples_split':[3,4,5,6,7], \n",
    "          'random_state':[123],\n",
    "          'n_jobs':[-1]}\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "#learning\n",
    "model1.fit(xTrain,yTrain)\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\",model1.best_params_)\n",
    "#Prediction\n",
    "prediction=model1.predict(xTest)\n",
    "#importing the metrics module\n",
    "from sklearn import metrics\n",
    "#evaluation(Accuracy)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(prediction,yTest))\n",
    "#evaluation(Confusion Metrix)\n",
    "print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "#clf = RandomForestClassifier(min_samples_leaf=2, min_samples_split=3, n_estimators=400, max_depth=30, bootstrap=False, criterion='gini')\n",
    "#clf.fit(X=train.drop(columns=['ID','IsSinkhole']), y=train.IsSinkhole)\n",
    "clf = RandomForestClassifier(min_samples_leaf=0.00000000001,n_jobs=-1,max_features='auto',min_samples_split=4, n_estimators=100, random_state= 123, max_depth=50, bootstrap=False, criterion='entropy')\n",
    "#clf.fit(X=train.drop(columns=['ID','IsSinkhole']), y=train.IsSinkhole)\n",
    "y_pred = clf.predict(test.drop(columns=['ID','IsSinkhole']))\n",
    "#Prediction\n",
    "#pred_val = np.argmax(pred_prob, axis = 1)\n",
    "#predictions = model_lgb.predict(test.drop(columns=['ID']),)\n",
    "test['IsSinkhole'] = y_pred\n",
    "test.to_csv('d8_decision_tree2.csv',index_label='ID',columns=['IsSinkhole'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

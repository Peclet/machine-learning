{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part_1_Logistic_Regression_using_PyTorch_(starter) (1) (1).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX2GTKPMyfkJ",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 3 (Part 1): Logistic Regression using PyTorch (Starter)\n",
        "\n",
        "* Student Name: **DAVID ADENIJI**\n",
        "* Enrollment Status: **GRADUATE STUDENT**\n",
        "\n",
        "In this assignment, you will fit logistic regression models to each of the six synthetic datasets you used for first assignment.  Your goal will be to obtained the lowest average test loss across all datasets using a single set of features and optimization parameters.\n",
        "\n",
        "Things to include in the writeup:\n",
        "\n",
        "* For the best model you found: \n",
        "   * The source code for your implementation of `_expand_features` in `FancyLogisticRegressionModel`.\n",
        "   * The source code for the \"Training parameters\".\n",
        "   * The visualization (image) of the final models fit by your method for each dataset.\n",
        "   * The final average test loss your model obtained.\n",
        "* If you did not obtain an average test loss below 0.1, then you must also include the following:\n",
        "   * The code (`_expand_features`) for at least three additional features spaces you tried along with their average test losses.\n",
        "   * A description of the range of \"Training parameters\" you tried (for example, I tried these four optimizers, this range of learning rates, and this range of epochs).\n",
        "\n",
        "Advice on installing PyTorch:\n",
        "\n",
        "* If you are setting this up in a virtual environment on your own machine, make sure you also install ipython and jupyter in the environment.  If you don't do this, then you may end up running the jupyter notebook from your base python installation.  This can cause problems when it tries to load the PyTorch in your virtual environment.\n",
        "\n",
        "Advice on finding a good set of features and training parameters:\n",
        "\n",
        "* It isn't required, but you could do a randomized parameter search to solve for the optimal model parameters.  The way that works is that you randomly pick a set of training parameters from within some range and train the model.  You do this repeatedly and keep the model that performs best your validation dataset.  The nice thing about this approach is that you can just start it running and come back in the morning to see the best model.\n",
        "* Make sure your features are relatively the same magnitude.  You may need to divide by a constant.\n",
        "* You may want to record and plot the loss at each iteration to help you debug.\n",
        "* Get your model working well on simple datasets first.\n",
        "* If increasing the number of epochs makes your model worse, then you probably should increase regularization.\n",
        "* You could speed up the overall training time by adding a convergence criteria, for example, if the loss doesn't go down by a certain amount for a few updates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IBYKTBuyvw-",
        "colab_type": "text"
      },
      "source": [
        "# Setup Environment\n",
        "\n",
        "This is designed to work in Google Colab.  You may need to do some of these from the command line if you are using your own installation of Jupyter/Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBvBadJNyb5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ImrUA-40FJMj",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJX0NBQF_IFh",
        "colab_type": "text"
      },
      "source": [
        "## Build some simple datasets\n",
        "\n",
        "These should look familiar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je46tOwP_MTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_from_mean_and_cov(means, covs, labels, mode='train', count=100):\n",
        "  \n",
        "  np.random.seed(0) if mode == 'train' else np.random.seed(1)\n",
        "      \n",
        "  vals = np.array([]).reshape(0,len(means[0])+1)\n",
        "\n",
        "  for i, (mean,cov,label) in enumerate(zip(means,covs,labels)):\n",
        "    vals_new = np.random.multivariate_normal(mean,cov,count)/10.;\n",
        "    vals_new = np.hstack([vals_new,np.ones((vals_new.shape[0],1))*label])\n",
        "    vals = np.vstack([vals,vals_new])    \n",
        "\n",
        "  df = pd.DataFrame(data=vals,columns=['x1','x2','y'])\n",
        "\n",
        "  return df\n",
        "\n",
        "# There are 6 different simple training datasets\n",
        "\n",
        "datasets = {\n",
        "    'train1':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[1,0]),\n",
        "    'train2':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[0,1]),\n",
        "    'train3':gen_from_mean_and_cov([[0,1],[0,3.5]],[[[1, .8],[.8, 1]],[[1, .8],[.8, 1]]],[1,0]),\n",
        "    'train4':gen_from_mean_and_cov([[1,0],[3,1]],[[[1, .8],[.8, 1]],[[1, .9],[.9, 1]]],[1,0]),\n",
        "    'train5':gen_from_mean_and_cov([[0,0],[4,0],[-4,0],[0,4],[0,-4]],[[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],[1,0,0,0,0]),\n",
        "    'train6':gen_from_mean_and_cov(\n",
        "    [[0,0],[4,0],[-4,0],[0,4],[0,-4],[-4,4],[4,4],[4,-4],[-4,-4]],\n",
        "    [[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],\n",
        "    [1,0,0,0,0,1,1,1,1]),\n",
        "    'test1':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[1,0],mode='test'),\n",
        "    'test2':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[0,1],mode='test'),\n",
        "    'test3':gen_from_mean_and_cov([[0,1],[0,3.5]],[[[1, .8],[.8, 1]],[[1, .8],[.8, 1]]],[1,0],mode='test'),\n",
        "    'test4':gen_from_mean_and_cov([[1,0],[3,1]],[[[1, .8],[.8, 1]],[[1, .9],[.9, 1]]],[1,0],mode='test'),\n",
        "    'test5':gen_from_mean_and_cov([[0,0],[4,0],[-4,0],[0,4],[0,-4]],[[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],[1,0,0,0,0],mode='test'),\n",
        "    'test6':gen_from_mean_and_cov(\n",
        "    [[0,0],[4,0],[-4,0],[0,4],[0,-4],[-4,4],[4,4],[4,-4],[-4,-4]],\n",
        "    [[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],\n",
        "    [1,0,0,0,0,1,1,1,1],mode='test')\n",
        "}\n",
        "\n",
        "df = datasets['train2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lid3m00jpYi6",
        "colab_type": "text"
      },
      "source": [
        "## Helpful Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nIfaPLXjGQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the classifer\n",
        "def vis2d_classifier(net,data):\n",
        "  \n",
        "  # create the domain for the plot\n",
        "  x1_min = data.x1.min()\n",
        "  x1_max = data.x1.max()\n",
        "  x2_min = data.x2.min()\n",
        "  x2_max = data.x2.max()\n",
        "\n",
        "  X1,X2 = np.meshgrid(\n",
        "      np.linspace(x1_min, x1_max, 200),\n",
        "      np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "  # convert it into a matrix (rows are locations, columns are features)\n",
        "  vis_data = np.hstack([X1.reshape(-1,1),X2.reshape(-1,1)])\n",
        "\n",
        "  # classify each location\n",
        "  vis_sco = net(torch.tensor(vis_data,dtype=torch.float32))\n",
        "\n",
        "  # convert back into image shapes\n",
        "  vis_sco = vis_sco.detach().numpy().reshape(X1.shape)\n",
        "  vis_class = vis_sco > .5;\n",
        "  \n",
        "  #\n",
        "  # Make the plots\n",
        "  #\n",
        "\n",
        "  # show the function value in the background\n",
        "  cs = plt.imshow(vis_sco,\n",
        "    extent=(x1_min,x1_max,x2_max,x2_min), # define limits of grid, note reversed y axis\n",
        "    cmap=plt.cm.jet, vmin=0.,vmax=1.)\n",
        "  plt.clim(0,1) # defines the value to assign the min/max color\n",
        "\n",
        "  # draw the line on top\n",
        "  levels = np.array([.5])\n",
        "  cs_line = plt.contour(X1,X2,vis_sco,levels, colors='k')\n",
        "\n",
        "  plt.scatter(data.x1,data.x2,c=data.y,edgecolors='w',cmap=plt.get_cmap('jet'))\n",
        "\n",
        "  # add a color bar\n",
        "  CB = plt.colorbar(cs)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wg1_Me7_J56",
        "colab_type": "text"
      },
      "source": [
        "# Setup the model\n",
        "\n",
        "** TASK: ** edit `_expand_features` so that this model works better.  See the last cell for a comparison to the best possible model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAcrEpT9yzZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FancyLogisticRegressionModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(FancyLogisticRegressionModel, self).__init__()\n",
        "\n",
        "        # compute the number of input features\n",
        "        self.input_dims = self._expand_features(torch.zeros(1,2)).shape[1]\n",
        "        \n",
        "        self.linear = nn.Linear(self.input_dims, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "    #\n",
        "    # perform basis expansion (change this so it work for all datasets!)\n",
        "    #\n",
        "    def _expand_features(self, x):\n",
        "            \n",
        "      # this currently constructs a 2nd-order polynomial model\n",
        "      #x_expanded = torch.stack([x[:,0], x[:,1], (x[:,0]**2)/2., (x[:,1]**2)/2., (x[:,0]*x[:,1])/2.,(x[:,0]**3)/3.,(x[:,1]**3)/3., (1 + x[:,0]*x[:,1])/3., (x[:,0]**4)/4.,(x[:,1]**4)/4., (1 + x[:,0]*x[:,1])/4.],dim=1)\n",
        "      x_expanded = torch.stack([x[:,0], x[:,1], (x[:,0]**2)/2., (x[:,1]**2)/2., (x[:,0]*x[:,1])**2., (x[:,0]**3),\n",
        "                                (x[:,1]**3), (1 + x[:,0]*x[:,1])**3., (x[:,0]**4), (x[:,1]**4), (1 + x[:,0]*x[:,1])**4.,\n",
        "                                 np.sqrt(x[:,0]**2+x[:,1]**2)],dim=1)\n",
        "      #x_expanded = torch.stack([x[:,0], x[:,1], (x[:,0]**2)/2., (x[:,1]**2)/2., (x[:,0]*x[:,1])**2., (1 + x[:,0]*x[:,1])**2, ((1+(x[:,1])**2)*x[:,0])**2, ((1+(x[:,0])**2)*x[:,1])**2, (x[:,0]**3), (x[:,1]**3), (1 + x[:,0]*x[:,1])**3, (x[:,0]**4), (x[:,1]**4), (1 + x[:,0]*x[:,1])**4.],dim=1)\n",
        "      return x_expanded\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "      # Add basis expansion here, make sure to adjust the self.input_dims\n",
        "      # accordingly in __init__.\n",
        "      x_expanded = self._expand_features(x) # do basis expansion\n",
        "      logits = self.linear(x_expanded) # predict the logits\n",
        "      probs = self.sig(logits) # compute the probability for each example\n",
        "      return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD3TgNVa5zLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L9TrzPJ4p9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljxauIN1pfhl",
        "colab_type": "text"
      },
      "source": [
        "## Visualize a random classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Uzdd4jpeQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vis2d_classifier(FancyLogisticRegressionModel(),df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uo4_4lVEwXw",
        "colab_type": "text"
      },
      "source": [
        "# Optimize Networks for Each Dataset\n",
        "\n",
        "The code below optimizes the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6xiobLV_ZxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Training parameters (INCLUDE THESE IN YOUR WRITEUP)\n",
        "#\n",
        "\n",
        "learning_rate = 3\n",
        "max_epochs = 1200\n",
        "opt_method = torch.optim.Adam\n",
        "weight_decay = 1e-9\n",
        "#\n",
        "# You shouldn't modify anything below here, but you should definitely read it\n",
        "# to understand what is happening.\n",
        "#\n",
        "\n",
        "# Store average losses\n",
        "test_losses = []\n",
        "\n",
        "# Fit all models\n",
        "for data_id in range(1,7):\n",
        "\n",
        "  # gather the train/test datasets\n",
        "  df_train = datasets['train{}'.format(data_id)]\n",
        "  df_test = datasets['test{}'.format(data_id)]\n",
        "\n",
        "  # initialize a model\n",
        "  net = FancyLogisticRegressionModel()\n",
        "\n",
        "  # define the loss\n",
        "  loss_function = nn.BCELoss()\n",
        "\n",
        "  # define an optimizer\n",
        "  optimizer = opt_method(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "  # build the torch tensors\n",
        "  feats_train = torch.tensor(df_train[['x1','x2']].values, dtype=torch.float32);\n",
        "  vals_train = torch.tensor(df_train[['y']].values, dtype=torch.float32);\n",
        "\n",
        "  feats_test = torch.tensor(df_test[['x1','x2']].values,dtype=torch.float32)\n",
        "  vals_test = torch.tensor(df_test[['y']].values, dtype=torch.float32)\n",
        "  \n",
        "  # run gradient descent\n",
        "  for epoch in range(max_epochs):\n",
        "    \n",
        "    optimizer.zero_grad();               # Intialize the gradient accumulators to all zeros\n",
        "    outputs = net(feats_train);                # Forward pass: compute the output class given a image\n",
        "    loss = loss_function(outputs, vals_train); # Compute the loss: difference between the output class and the pre-given label\n",
        "    loss.backward();                     # Backward pass: compute the weight\n",
        "    optimizer.step();                    # Optimizer: update the weights of hidden nodes\n",
        "    \n",
        "  # compute and store test loss\n",
        "  test_loss = loss_function(net(feats_test), vals_test)\n",
        "  test_losses.append(float(test_loss))\n",
        "    \n",
        "  print('Dataset %d, Epoch [%d/%d], Train Loss: %.4f, Test Loss: %.4f' % (data_id, epoch+1, max_epochs, float(loss), float(test_loss)))\n",
        "\n",
        "  plt.figure()\n",
        "  vis2d_classifier(net,df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfA-6ieVWYWG",
        "colab_type": "text"
      },
      "source": [
        "## Compute the Average Test Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYO2Ja9eWB4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The average test loss across all datasets for you model was {:2.3}.'.format(np.array(test_losses).mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCMWD-flobxH",
        "colab_type": "text"
      },
      "source": [
        "The best average loss I obtained was around 0.084 (although this varies a bit every time due to the random initialization).  Can you do better?"
      ]
    }
  ]
}
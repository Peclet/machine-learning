{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "doad Part_2_Neural_Networks_using_PyTorch_(starter)_(1) (1).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX2GTKPMyfkJ",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 3 (Part 2): Neural Networks using PyTorch (Starter)\n",
        "\n",
        "* Student Name: **DAVID ADENIJI**\n",
        "* Enrollment Status: **GRADUATE**\n",
        "\n",
        "In this assignment, you will fit neural network models to each of the six synthetic datasets you used for first assignment.  Your goal will be to obtained the lowest average test loss across all datasets using a single neural network architecture and set of optimization parameters.\n",
        "\n",
        "Things to include in the writeup:\n",
        "\n",
        "* For the best model you found:\n",
        "  * The source code for your \"Model Architecture Parameters\".\n",
        "  * The source code for the \"Training Parameters\".\n",
        "  * For each dataset, the plot of the training error and the visualization of the final classifier (feel free to cut and paste).\n",
        "  * The final average test loss your model obtained (feel free to cut and paste).\n",
        "* If you did not obtain an average test loss below 0.1, then you must also include the following:\n",
        "   * The code for at least three additional \"Model Architecture Parameters\" that you tried along with their average test losses. \n",
        "   * A description of the process you used to try to set the \"Training Parameters\" and \"Model Architecture Parameters\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IBYKTBuyvw-",
        "colab_type": "text"
      },
      "source": [
        "# Setup Environment\n",
        "\n",
        "This is designed to work in Google Colab.  You may need to do some of these from the command line if you are using your own installation of Jupyter/Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBvBadJNyb5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ImrUA-40FJMj",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJX0NBQF_IFh",
        "colab_type": "text"
      },
      "source": [
        "## Build some simple datasets\n",
        "\n",
        "These should look familiar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je46tOwP_MTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_from_mean_and_cov(means, covs, labels, mode='train', count=100):\n",
        "  \n",
        "  np.random.seed(0) if mode == 'train' else np.random.seed(1)\n",
        "      \n",
        "  vals = np.array([]).reshape(0,len(means[0])+1)\n",
        "\n",
        "  for i, (mean,cov,label) in enumerate(zip(means,covs,labels)):\n",
        "    vals_new = np.random.multivariate_normal(mean,cov,count)/10.;\n",
        "    vals_new = np.hstack([vals_new,np.ones((vals_new.shape[0],1))*label])\n",
        "    vals = np.vstack([vals,vals_new])    \n",
        "\n",
        "  df = pd.DataFrame(data=vals,columns=['x1','x2','y'])\n",
        "\n",
        "  return df\n",
        "\n",
        "# There are 6 different simple training datasets\n",
        "\n",
        "datasets = {\n",
        "    'train1':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[1,0]),\n",
        "    'train2':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[0,1]),\n",
        "    'train3':gen_from_mean_and_cov([[0,1],[0,3.5]],[[[1, .8],[.8, 1]],[[1, .8],[.8, 1]]],[1,0]),\n",
        "    'train4':gen_from_mean_and_cov([[1,0],[3,1]],[[[1, .8],[.8, 1]],[[1, .9],[.9, 1]]],[1,0]),\n",
        "    'train5':gen_from_mean_and_cov([[0,0],[4,0],[-4,0],[0,4],[0,-4]],[[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],[1,0,0,0,0]),\n",
        "    'train6':gen_from_mean_and_cov(\n",
        "    [[0,0],[4,0],[-4,0],[0,4],[0,-4],[-4,4],[4,4],[4,-4],[-4,-4]],\n",
        "    [[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],\n",
        "    [1,0,0,0,0,1,1,1,1]),\n",
        "    'test1':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[1,0],mode='test'),\n",
        "    'test2':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[0,1],mode='test'),\n",
        "    'test3':gen_from_mean_and_cov([[0,1],[0,3.5]],[[[1, .8],[.8, 1]],[[1, .8],[.8, 1]]],[1,0],mode='test'),\n",
        "    'test4':gen_from_mean_and_cov([[1,0],[3,1]],[[[1, .8],[.8, 1]],[[1, .9],[.9, 1]]],[1,0],mode='test'),\n",
        "    'test5':gen_from_mean_and_cov([[0,0],[4,0],[-4,0],[0,4],[0,-4]],[[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],[1,0,0,0,0],mode='test'),\n",
        "    'test6':gen_from_mean_and_cov(\n",
        "    [[0,0],[4,0],[-4,0],[0,4],[0,-4],[-4,4],[4,4],[4,-4],[-4,-4]],\n",
        "    [[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],\n",
        "    [1,0,0,0,0,1,1,1,1],mode='test')\n",
        "}\n",
        "\n",
        "df = datasets['train1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lid3m00jpYi6",
        "colab_type": "text"
      },
      "source": [
        "## Helpful Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nIfaPLXjGQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the classifer\n",
        "def vis2d_classifier(net,data):\n",
        "  \n",
        "  # create the domain for the plot\n",
        "  x1_min = data.x1.min()\n",
        "  x1_max = data.x1.max()\n",
        "  x2_min = data.x2.min()\n",
        "  x2_max = data.x2.max()\n",
        "\n",
        "  X1,X2 = np.meshgrid(\n",
        "      np.linspace(x1_min, x1_max, 200),\n",
        "      np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "  # convert it into a matrix (rows are locations, columns are features)\n",
        "  vis_data = np.hstack([X1.reshape(-1,1),X2.reshape(-1,1)])\n",
        "\n",
        "  # classify each location\n",
        "  vis_sco = net.eval()(torch.tensor(vis_data,dtype=torch.float32))\n",
        "\n",
        "  # convert back into image shapes\n",
        "  vis_sco = vis_sco.detach().numpy().reshape(X1.shape)\n",
        "  vis_class = vis_sco > .5;\n",
        "  \n",
        "  #\n",
        "  # Make the plots\n",
        "  #\n",
        "\n",
        "  # show the function value in the background\n",
        "  cs = plt.imshow(vis_sco,\n",
        "    extent=(x1_min,x1_max,x2_max,x2_min), # define limits of grid, note reversed y axis\n",
        "    cmap=plt.cm.jet, vmin=0.,vmax=1.)\n",
        "  plt.clim(0,1) # defines the value to assign the min/max color\n",
        "\n",
        "  # draw the line on top\n",
        "  levels = np.array([.5])\n",
        "  cs_line = plt.contour(X1,X2,vis_sco,levels, colors='k')\n",
        "\n",
        "  plt.scatter(data.x1,data.x2,c=data.y,edgecolors='w',cmap=plt.get_cmap('jet'))\n",
        "\n",
        "  # add a color bar\n",
        "  CB = plt.colorbar(cs)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wg1_Me7_J56",
        "colab_type": "text"
      },
      "source": [
        "# Setup the model\n",
        "\n",
        "** TASK: ** Your job is to define a neural network that performs well on this dataset.  See the last cell for a comparison to the best possible model.  You will need to find a good combination of training parameters and model parameters.  You don't actually need to modify this code, just define a good set of parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAcrEpT9yzZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_network(hidden_nodes=[6,6], activation_fn=nn.ReLU, include_batchnorm=True):\n",
        "\n",
        "  layers = []\n",
        "\n",
        "  prev_nodes = 2 # 2 input values\n",
        "  for out_nodes in hidden_nodes:\n",
        "    layers.append(nn.Linear(prev_nodes, out_nodes))\n",
        "    if include_batchnorm:\n",
        "      layers.append(nn.BatchNorm1d(out_nodes))\n",
        "    layers.append(activation_fn())\n",
        "    \n",
        "    prev_nodes = out_nodes\n",
        "\n",
        "  layers.append(nn.Linear(prev_nodes,1))\n",
        "  layers.append(nn.Sigmoid())\n",
        "    \n",
        "  return torch.nn.Sequential(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljxauIN1pfhl",
        "colab_type": "text"
      },
      "source": [
        "## Visualize a random classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Uzdd4jpeQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vis2d_classifier(build_network(),df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uo4_4lVEwXw",
        "colab_type": "text"
      },
      "source": [
        "# Optimize Networks for Each Dataset\n",
        "\n",
        "The code below optimizes the model.  It works as follows:\n",
        "\n",
        "* It computes gradient updates on mini-batches of training examples.  Each mini-batch contains `batch_size` examples.\n",
        "* There are five training rounds, and it runs a fixed number of epochs per training round (`epochs_per_round`).\n",
        "* After each training round, the learning rate is reduced by a multiplicative factor (`learning_rate_decay`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6xiobLV_ZxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Training Parameters (INCLUDE THESE IN YOUR WRITEUP)\n",
        "#\n",
        "\n",
        "opt_method = torch.optim.Adam\n",
        "learning_rate = .01\n",
        "batch_size = 5 # number of examples to use to compute each gradient update\n",
        "epochs_per_round = 40 # number of times to loop over the dataset \n",
        "weight_decay = 0.1e-7\n",
        "learning_rate_decay = .01 # amount to decrease learning rate after each training round\n",
        "\n",
        "#\n",
        "# Model Architecture Parameters (INCLUDE THESE IN YOUR WRITEUP)\n",
        "#\n",
        "\n",
        "activation_fn = nn.ReLU\n",
        "include_batchnorm = False\n",
        "hidden_nodes=[50,30,20]\n",
        "\n",
        "#\n",
        "# You shouldn't modify anything below here, but you should definitely read it\n",
        "# to understand what is happening.\n",
        "#\n",
        "\n",
        "# Store average losses\n",
        "test_losses = []\n",
        "\n",
        "# Fit models to each datasets\n",
        "for data_id in range(1,7):\n",
        "\n",
        "  # gather the train/test datasets\n",
        "  df_train = datasets['train{}'.format(data_id)]\n",
        "  df_test = datasets['test{}'.format(data_id)]\n",
        "\n",
        "  # initialize a model\n",
        "  net = build_network(hidden_nodes=hidden_nodes, activation_fn=activation_fn, include_batchnorm=include_batchnorm)\n",
        "\n",
        "  # define the loss\n",
        "  loss_function = nn.BCELoss()\n",
        "\n",
        "  # build the torch tensors\n",
        "  train_dataset = TensorDataset(\n",
        "      torch.tensor(df_train[['x1','x2']].values,dtype=torch.float32),\n",
        "      torch.tensor(df_train[['y']].values,dtype=torch.float32)\n",
        "  )\n",
        "  \n",
        "  feats_test = torch.tensor(df_test[['x1','x2']].values,dtype=torch.float32)\n",
        "  vals_test = torch.tensor(df_test[['y']].values, dtype=torch.float32)\n",
        "\n",
        "  # use this to yield random mini-batches\n",
        "  train_loader = DataLoader(dataset=train_dataset,\n",
        "                            batch_size=batch_size, \n",
        "                            shuffle=True)\n",
        "  \n",
        "  training_loss_per_epoch = []\n",
        "\n",
        "  # run gradient descent for multiple rounds, decreasing learning rate each time\n",
        "  for training_round in range(5):\n",
        "\n",
        "    optimizer = opt_method(net.parameters(), lr=learning_rate*learning_rate_decay**(training_round), weight_decay=weight_decay)\n",
        "    \n",
        "    for epoch in range(epochs_per_round):\n",
        "      \n",
        "      loss_accum = 0.0\n",
        "      \n",
        "      # iterate over mini-batches\n",
        "      for i_batch, (feats, vals) in enumerate(train_loader):\n",
        "\n",
        "        optimizer.zero_grad();               # Intialize the gradient accumulators to all zeros\n",
        "        outputs = net(feats);                # Forward pass: compute the output class given a image\n",
        "        loss = loss_function(outputs, vals); # Compute the loss: difference between the output class and the pre-given label\n",
        "        loss.backward();                     # Backward pass: compute the weight\n",
        "        optimizer.step();                    # Optimizer: update the weights of hidden nodes\n",
        "        \n",
        "        loss_accum += float(loss)\n",
        "\n",
        "      training_loss_per_epoch.append(loss_accum/i_batch)\n",
        "  \n",
        "  # compute and store test loss\n",
        "  test_loss = loss_function(net(feats_test), vals_test)\n",
        "  test_losses.append(float(test_loss))    \n",
        "  print('Dataset %d, Round [%d], Epoch [%d/%d], Train Loss: %.4f, Test Loss: %.4f' % (data_id, training_round, epoch+1, epochs_per_round, float(loss), float(test_loss)))\n",
        "\n",
        "  # plot training loss to help diagnose convergence issues\n",
        "  plt.figure()\n",
        "  plt.plot(training_loss_per_epoch)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Average Training Loss')\n",
        "\n",
        "  # show the final classifier\n",
        "  plt.figure()\n",
        "  vis2d_classifier(net,df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfA-6ieVWYWG",
        "colab_type": "text"
      },
      "source": [
        "## Compute the Average Test Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYO2Ja9eWB4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The average test loss across all datasets for your model was {:2.3}.'.format(np.array(test_losses).mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCMWD-flobxH",
        "colab_type": "text"
      },
      "source": [
        "The best average loss I obtained was around 0.078 (although this varies a bit every time due to the random initialization).  Can you do better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUiTT_abi7Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}